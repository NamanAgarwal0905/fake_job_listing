{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPfaVxB+Eyx7bVQt4E3K+pP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"2kZD8pBnfqmj","executionInfo":{"status":"ok","timestamp":1724514521760,"user_tz":-330,"elapsed":3481759,"user":{"displayName":"Naman Agarwal","userId":"14334788443247473687"}},"outputId":"1f3db1c3-7717-4d9b-cabb-063fc5df89fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Dataset loaded in 0.36 seconds\n","Checking for missing values...\n","1\n","Splitting dataset...\n","Dataset split completed in 0.00 seconds\n","Initializing BERT tokenizer and model...\n","Tokenizer and model initialization completed in 1.57 seconds\n","Extracting BERT features...\n","BERT feature extraction completed in 3478.97 seconds\n","Extracting additional features...\n","Additional feature extraction completed in 0.01 seconds\n","Concatenating BERT embeddings with additional features...\n","Feature concatenation completed in 0.10 seconds\n","Scaling features with MinMaxScaler...\n","Feature scaling completed in 0.09 seconds\n","Training Naive Bayes classifier...\n","Training completed in 0.06 seconds\n","Making predictions on the test set...\n","Prediction completed in 0.00 seconds\n","Evaluating the model...\n","Evaluation completed in 0.02 seconds\n","Test Accuracy: 0.9105145413870246\n","Test Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95       418\n","           1       0.00      0.00      0.00        29\n","\n","    accuracy                           0.91       447\n","   macro avg       0.47      0.49      0.48       447\n","weighted avg       0.87      0.91      0.89       447\n","\n","Saving model and scaler...\n","Model and scaler saved in 0.01 seconds\n","Download the files:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f1a818a6-00e1-4e19-8794-ef8001709fb6\", \"naive_bayes_model.pkl\", 25463)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_a8cbd2ed-70d0-4792-9a71-e0a6941278d3\", \"standard_scaler.pkl\", 31519)"]},"metadata":{}}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import BertModel, BertTokenizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import MinMaxScaler\n","import time\n","import joblib  # Import joblib for saving models\n","from google.colab import files\n","\n","# Function to process data in batches\n","def extract_features_in_batches(descriptions, tokenizer, model, device, batch_size=32, max_len=128):\n","    features = []\n","    num_batches = len(descriptions) // batch_size + (1 if len(descriptions) % batch_size > 0 else 0)\n","\n","    for i in range(num_batches):\n","        batch_descs = descriptions[i * batch_size: (i + 1) * batch_size]\n","        batch_features = extract_bert_features(batch_descs, tokenizer, model, device, max_len)\n","        features.extend(batch_features)\n","\n","    return features\n","\n","def extract_bert_features(texts, tokenizer, model, device, max_len=128):\n","    tokens = tokenizer(\n","        texts.tolist(),\n","        max_length=max_len,\n","        padding=True,\n","        truncation=True,\n","        return_tensors='pt'\n","    ).to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(**tokens)\n","\n","    # Use [CLS] token hidden state as the sentence embedding\n","    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","    return cls_embeddings\n","\n","# Load your dataset from runtime\n","print(\"Loading dataset...\")\n","start_time = time.time()\n","df = pd.read_csv('/content/job_train.csv')  # Ensure the file is in the Colab runtime\n","print(f\"Dataset loaded in {time.time() - start_time:.2f} seconds\")\n","\n","# Check for missing values in 'description' column\n","print(\"Checking for missing values...\")\n","print(df['description'].isnull().sum())\n","\n","# Handle missing values in 'description' column\n","df['description'].fillna('missing', inplace=True)  # Option to fill NaN with 'missing'\n","\n","# Split dataset into training and test sets (5% for test set)\n","print(\"Splitting dataset...\")\n","start_time = time.time()\n","train_df, test_df = train_test_split(df, test_size=0.05, random_state=42)\n","print(f\"Dataset split completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Initialize the BertTokenizer and BERT model\n","print(\"Initializing BERT tokenizer and model...\")\n","start_time = time.time()\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","bert_model.eval()  # Set BERT to evaluation mode\n","device = torch.device('cpu')  # Use CPU\n","bert_model = bert_model.to(device)\n","print(f\"Tokenizer and model initialization completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Extract BERT features in batches\n","print(\"Extracting BERT features...\")\n","start_time = time.time()\n","train_bert_features = extract_features_in_batches(train_df['description'].to_numpy(), tokenizer, bert_model, device)\n","test_bert_features = extract_features_in_batches(test_df['description'].to_numpy(), tokenizer, bert_model, device)\n","print(f\"BERT feature extraction completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Extract additional binary features\n","print(\"Extracting additional features...\")\n","start_time = time.time()\n","train_additional_features = train_df[['has_company_logo', 'has_questions', 'telecommuting']].to_numpy()\n","test_additional_features = test_df[['has_company_logo', 'has_questions', 'telecommuting']].to_numpy()\n","print(f\"Additional feature extraction completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Concatenate BERT embeddings with additional features\n","print(\"Concatenating BERT embeddings with additional features...\")\n","start_time = time.time()\n","train_features = np.array([np.concatenate([bert_feat, add_feat]) for bert_feat, add_feat in zip(train_bert_features, train_additional_features)])\n","test_features = np.array([np.concatenate([bert_feat, add_feat]) for bert_feat, add_feat in zip(test_bert_features, test_additional_features)])\n","print(f\"Feature concatenation completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Scale the features using MinMaxScaler to ensure non-negative values\n","print(\"Scaling features with MinMaxScaler...\")\n","start_time = time.time()\n","scaler = MinMaxScaler()\n","train_features = scaler.fit_transform(train_features)\n","test_features = scaler.transform(test_features)\n","print(f\"Feature scaling completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Train the Naive Bayes classifier\n","print(\"Training Naive Bayes classifier...\")\n","start_time = time.time()\n","nb_model = MultinomialNB()\n","nb_model.fit(train_features, train_df['fraudulent'])\n","print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Make predictions on the test set\n","print(\"Making predictions on the test set...\")\n","start_time = time.time()\n","test_predictions = nb_model.predict(test_features)\n","print(f\"Prediction completed in {time.time() - start_time:.2f} seconds\")\n","\n","# Evaluate the model\n","print(\"Evaluating the model...\")\n","start_time = time.time()\n","accuracy = accuracy_score(test_df['fraudulent'], test_predictions)\n","report = classification_report(test_df['fraudulent'], test_predictions)\n","print(f\"Evaluation completed in {time.time() - start_time:.2f} seconds\")\n","\n","print(f'Test Accuracy: {accuracy}')\n","print(f'Test Report:\\n{report}')\n","\n","# Save the trained models and scaler\n","print(\"Saving model and scaler...\")\n","start_time = time.time()\n","joblib.dump(nb_model, '/content/naive_bayes_model.pkl')\n","joblib.dump(scaler, '/content/standard_scaler.pkl')\n","print(f\"Model and scaler saved in {time.time() - start_time:.2f} seconds\")\n","\n","print(\"Download the files:\")\n","files.download('/content/naive_bayes_model.pkl')\n","files.download('/content/standard_scaler.pkl')\n"]}]}